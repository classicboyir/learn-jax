{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static vs Traced Values and Operations\n",
    "\n",
    "Traced operations or values are a wal for XLA Compiler to know whether a process needs to be recompiled or not. This is being practiced below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1669963077.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    Traced operations or values are a wal for XLA Compiler to know whether a process needs to be recompiled or not. This is being practiced below:\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import jit\n",
    "from jax import make_jaxpr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will break, as the value of neg is not clear at the compile time and the result depends on the neg parameter. Therefore, it canot be traced, if we define it as a static parameter, then it will be skiped and XLA will only recompile it if the value changes, not the shape or type, but the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def f(x, neg):\n",
    "  print('hello')\n",
    "  return -x if neg else x\n",
    "\n",
    "f(1, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argument `static_argnums` indicates that the second parameter, (neg in this case) is static, in other words, this value is not traced and it will recompile if the value changes. Therefore, in this case, the shape and type of this parameter is not being traced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "@partial(jit, static_argnums=(1,))\n",
    "def f(x, neg):\n",
    "  print('x: ', x)\n",
    "  print('neg: ', neg)\n",
    "  return -x if neg else x\n",
    "\n",
    "f(1, True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(1, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Operations:\n",
    "\n",
    "Static operations are evaluated at compile time - traced operations are compiled and evaluated at runtime.\n",
    "\n",
    "For example, jnp is when you want an opearion to be traced and use np if you want the operation to be static."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Side effects and functional programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 15.\n",
    "def impure_uses_globals(x):\n",
    "  return x + g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JAX captures the value of the global during the first run\n",
    "print (\"First call: \", jit(impure_uses_globals)(4.))\n",
    "g = 10.  # Update the global\n",
    "\n",
    "# Subsequent runs may silently use the cached value of the globals\n",
    "print (\"Second call: \", jit(impure_uses_globals)(5.))\n",
    "\n",
    "# JAX re-runs the Python function when the type or shape of the argument changes\n",
    "# This will end up reading the latest value of the global\n",
    "print (\"Third call, different type: \", jit(impure_uses_globals)(jnp.array([4.])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = 10.\n",
    "def impure_saves_global(x):\n",
    "    global g\n",
    "    g = x + g\n",
    "    return x\n",
    "\n",
    "print(jit(impure_saves_global)(4.))\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax.lax as lax\n",
    "from jax import make_jaxpr\n",
    "\n",
    "# lax.fori_loop\n",
    "array = jnp.arange(10)\n",
    "print(lax.fori_loop(0, 10, lambda i,x: x+array[i], 0)) # expected result 45\n",
    "iterator = iter(range(10))\n",
    "print(lax.fori_loop(0, 10, lambda i,x: x+next(iterator), 0)) # unexpected result 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lax.scan\n",
    "def func11(arr, extra):\n",
    "    ones = jnp.ones(arr.shape)\n",
    "    def body(carry, aelems):\n",
    "        ae1, ae2 = aelems\n",
    "        return (carry + ae1 * ae2 + extra, carry)\n",
    "    return lax.scan(body, 0., (arr, ones))\n",
    "make_jaxpr(func11)(jnp.arange(16), 5.)\n",
    "# make_jaxpr(func11)(iter(range(16)), 5.) # throws error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lax.cond\n",
    "array_operand = jnp.array([0.])\n",
    "lax.cond(True, lambda x: x+1, lambda x: x-1, array_operand)\n",
    "iter_operand = iter(range(10))\n",
    "# lax.cond(True, lambda x: next(x)+1, lambda x: next(x)-1, iter_operand) # throws error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_jaxpr(func11)(iter(range(16)), 5.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelism across all the available devices with `pmap`\n",
    "\n",
    "`vmap` vectorizes in one devices while pmap distributes a job across all the devices, or it's a device parallelism approach. It implements the `SPMD` but sharding the data across add devices and run a function on the sharded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "x = np.arange(5)\n",
    "w = np.array([2., 3., 4.])\n",
    "\n",
    "def convolve(x, w):\n",
    "  output = []\n",
    "  for i in range(1, len(x)-1):\n",
    "    output.append(jnp.dot(x[i-1:i+2], w))\n",
    "  return jnp.array(output)\n",
    "\n",
    "convolve(x, w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_devices = jax.local_device_count() \n",
    "xs = np.arange(5 * n_devices).reshape(-1, 5)\n",
    "ws = np.stack([w] * n_devices)\n",
    "\n",
    "xs, ws\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's vectorize the convolve operation\n",
    "\n",
    "jax.vmap(convolve)(xs, ws)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this runs the function across devices:\n",
    "jax.pmap(convolve)(xs, ws)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returned array is already sharded across all devices, if we run another pmap operation on the returned array, the sharded data stays on the corresponding devices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.pmap(convolve)(xs, jax.pmap(convolve)(xs, ws))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to split the data across the nodes, `pmap` allows you to shard the data across the devices but the `in_axes` parameter.\n",
    "\n",
    "The xs argument is split along its leading axis (axis 0), meaning each device gets a different slice of xs. The w argument, however, is broadcast to all devices (indicated by None in in_axes), meaning each device uses the same w value. This is an alternative approach to manually replicating w across devices, simplifying the code and potentially reducing memory usage.\n",
    "\n",
    "The output shown is the result of this parallel computation, indicating that each device has correctly performed its part of the computation using its slice of xs and the broadcasted w, leading to an efficiently computed result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jax.pmap(convolve, in_axes=(0, None))(xs, w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, given this input matrix for xs:\n",
    "\n",
    "```json\n",
    "array([[ 0,  1,  2,  3,  4],\n",
    "       [ 5,  6,  7,  8,  9],\n",
    "       [10, 11, 12, 13, 14],\n",
    "       [15, 16, 17, 18, 19]])\n",
    "```\n",
    "\n",
    "Given we have 4 devices, the matrix sharding is as follows\n",
    "\n",
    "```json\n",
    "Device 1 would receive [0, 1, 2, 3, 4]\n",
    "Device 2 would receive [5, 6, 7, 8, 9]\n",
    "Device 3 would receive [10, 11, 12, 13, 14]\n",
    "Device 4 would receive [15, 16, 17, 18, 19]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
